# -*- coding: utf-8 -*-
"""DataCamp - CreditCard.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cT3PdKSCzDdVxCoqCVRMhrgjZovtysQ6

### Context

Commercial banks receive _a lot_ of applications for credit cards. Many of them get rejected for many reasons, like high loan balances, low income levels, or too many inquiries on an individual's credit report, for example. Manually analyzing these applications is mundane, error-prone, and time-consuming (and time is money!). Luckily, this task can be automated with the power of machine learning and pretty much every commercial bank does so nowadays. In this workbook, you will build an automatic credit card approval predictor using machine learning techniques, just like real banks do.


### The Data

The data is a small subset of the Credit Card Approval dataset from the UCI Machine Learning Repository showing the credit card applications a bank receives. This dataset has been loaded as a `pandas` DataFrame called `cc_apps`. The last column in the dataset is the target value.


### Steps Taken
In this Project, I took steps below to train the model and make prediction.


*   **Data Exploration**
  * Analyzed data types for each column to understand the structure of the dataset.
  * Investigated how missing values were represented, ensuring clarity on data integrity.
*   **Data Cleaning**
  * Train Test Split
  * Fill missing values in the training set appropriately to avoid data leakage.
  * Standardized numerical features based on the training set to ensure consistent scaling and avoid data leakage.
  * One-hot encoded categorical features to convert them into a format suitable for modeling.
* **Model Training**
  * Instantiate a Logistic Regression
  * Define a param grid with hyper parameters
  * Implemented GridSearchCV to evaluate different hyperparameter combinations, ensuring robust model performance through cross-validation.
  * Assessed the best model based on F1 score to validate performance on unseen data.

## Import Libararies & Preview Dara
"""

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import GridSearchCV

# Load the dataset
cc_apps = pd.read_csv("creditcard.csv", header=None)
cc_apps = cc_apps.drop(columns = 0)
cc_apps = cc_apps.drop(index = 0)
cc_apps.head()

"""## EDA
### 1. Exloring Data Types
### 2. Exploring Missing Values
### 3. Pre-Missing Values Handling
### 4. Convert Data Types

"""

# Examine missing values
missing_values = cc_apps.isna().sum()
print('Missing Values \n', missing_values[missing_values > 0], '\n')

# Examine column types
print('Column Types \n', cc_apps.dtypes, '\n')

# check unique value of non-numerical columns
non_numerical_columns = list(cc_apps.select_dtypes(exclude=['int64', 'float64']).columns)
print(cc_apps[non_numerical_columns].apply(lambda x: x.unique()),'\n')

#columns with missing value (?)
columns_with_question_mark = list(cc_apps.columns[cc_apps.apply(lambda col: col.astype(str).str.contains('\?').any())])
print('Columns with Missing Value ? \n',columns_with_question_mark,'\n')

#replace all ? as na
cc_apps.replace('?', np.nan, inplace=True)

#cast column 1 to float64
cc_apps[1] = cc_apps[1].astype(float)

"""## Train Test Split"""

# train_test_split
X = cc_apps.drop(columns = 13)
y = cc_apps[13].map({'+': 1, '-': 0}) #map with binary
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, random_state = 1)

"""## Data Cleaning I
### 1. Dividing Features to **Numerical** & **Categorical**
### 2. Fill NAs using mean for numerical featrues based on traning set.
### 3. Fill NAs using mode for categorical features based on traning set.
### 4. Transform test set used mean & mode from training set.
"""

#Filling missing values using X_train's data, avoiding test data leakage
for col in X_train.columns:
    if X_train[col].dtype == 'object': #non-numerical columns
        mode = X_train[col].mode()[0]
        X_train[col].fillna(mode, inplace = True)
        X_test[col].fillna(mode, inplace = True)
    else:
        mean = X_train[col].mean()
        X_train[col].fillna(mean, inplace = True)
        X_test[col].fillna(mean, inplace = True)
print(X_train)
print(X_test)
print(X_train.isna().sum())
print(X_test.isna().sum())

"""## Data Cleaning II
### 1. Dividing Features to **Numerical** & **Categorical**
### 2. Standardize numerical features based on training set, transform test set
### 3. One Hot Encoding for categorical features.
### Final training & testing set ready.
"""

#Store Numerical & Non-Numerical Columns
numerical_cols = list(X_train.select_dtypes(include=['float64', 'int64']).columns)
non_numerical_cols = list(X_train.select_dtypes(exclude=['float64', 'int64']).columns)
print(numerical_cols)
print(non_numerical_cols)

#Standardize Numerical Columns based on Training set
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train[numerical_cols])
X_test_scaled = scaler.transform(X_test[numerical_cols])

# Convert scaled data back to DataFrame
X_train_scaled = pd.DataFrame(X_train_scaled, columns=numerical_cols, index=X_train.index)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=numerical_cols, index=X_test.index)

# Combine with original categorical data
X_train_scaled_w_cat = pd.concat([X_train_scaled, X_train[non_numerical_cols]], axis=1)
X_test_scaled_w_cat = pd.concat([X_test_scaled, X_test[non_numerical_cols]], axis=1)
print(X_train_scaled_w_cat.shape)
print(X_test_scaled_w_cat.shape)

# transform categorical data into binaries
X_train_final = pd.get_dummies(X_train_scaled_w_cat, columns=non_numerical_cols, drop_first=True)
X_test_final = pd.get_dummies(X_test_scaled_w_cat, columns=non_numerical_cols, drop_first=True)

# Align test set to have the same columns as train set, filling missing columns with 0
X_test_final = X_test_final.reindex(columns=X_train_final.columns, fill_value=0)

print(X_train_final.columns)
print(X_test_final.columns)
print(X_train_final.head())
print(X_test_final.head())

"""## Train Logistic Regression"""

# Assuming X_train_final and X_test_final are pandas DataFrames
# Convert column names to strings
X_train_final.columns = X_train_final.columns.astype(str)
X_test_final.columns = X_test_final.columns.astype(str)

logreg = LogisticRegression()

# Define Grid
param_grid = {
    'C': [0.01, 0.1, 1, 10],
    'penalty': ['l1', 'l2'],
    'solver': ['liblinear', 'saga']
}

# GridSearchCV
grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='f1', n_jobs=-1) #in business contest, assuming FP and FN both costs
grid_search.fit(X_train_final, y_train)


# Evaluate best model
print("Best Params:", grid_search.best_params_)
print("Best CV Score:", grid_search.best_score_)
print("Test Accuracy:", grid_search.score(X_test_final, y_test))

"""Best Params: {'C': 1, 'penalty': 'l1', 'solver': 'saga'}

Best CV Score: 0.8566682034207005

Test Accuracy: 0.8615384615384616



"""